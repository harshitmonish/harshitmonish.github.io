---
title: 'What are Transformer models?'
date: 2022-10-01
permalink: /posts/2022/10/blog-post-3/
tags:
  - Artificial Intelligence
  - Machine Learning
  - Transformer model
---
Transformer Model Introduction
===
<figure>
<img src='/images/transformers.png' width="75%" height="50%">
<figcaption align="center"><b> The  encoder-decoder structure of the Transformer architecture taken from <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></b></figcaption>
</figure>

A transformer model is a deep neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence. It adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. 
Like recurrent neural networks (RNNs), LSTMs,  transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs that are slow to train and the input data needs to be passed sequentially, transformers process the entire input all at once in parallel. 

It applies an evolving set of mathematical techniques called attention or self-attention to detect subtle ways even distant data elements in a series depend on each other. 
Transformers were introduced in 2017 in the paper "[Attention is All You Need](https://arxiv.org/abs/1706.03762)" by a team at Google Brain and are increasingly the model of choice for NLP problems, replacing RNN models such as long short-term memory (LSTM). Transformer network architecture employs a encoder decoder architecture much like RNNs but, it allows to pass the input sequence in parallel which allow training parallelization allows on larger datasets. The task of the encoder is to map an input sequence to a sequence of continuous representations which is fed into decoder and the decoder receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence.

Deep Dive into Transformer model blocks
===
## Positional Encoding ##
First thing that we do is generate input embeddings of the text to map every word to a point in space where similar words are physically closer to each other. We could use pretrained embeddings of words to save time i.e. GLOVe, ELMO embeddings. The paper uses the sine and cosine function to generate the word embeddings. This embedding maps word to a vector, same word may have different meaning in different context in a sentence and this is where positional embedding comes into play. If we don't use positional encoding then we are treating all words as bag of words instead of sequence of words. After having input embedding and applying the positional encoding we get the word vectors that have positional information i.e. context.

## Attention in Machine Learning ##
Before getting into the details of the encoder block, let's talk first about the concept of attention. Attention is the ability to dynamically highlight and use the salient parts of the information at hand, it involves answering what part of input should I be focusing on - in a similar manner as it does in human brain. It was introduced by [Bahdanau et al. (2014)](https://arxiv.org/abs/1409.0473) to address the bottleneck problem that arises with the use of a fixed-length encoding vector, where the decoder would have limited access to the information provided by the input. The attention mechanism described in the paper for RNNs is divided into three steps:
* **Alignment scores**: It takes the encoded hidden states ${h_i}$ and the previous decoder output, ${s_{i-1}}$ to compute a score ${e_{t,i}}$ that indicates how well the elements of the input sequence align with the current output at the position, t. The alignment model is represented by a function, which can be implemented by a feedforward neural network: ${e_{t,i}} = a({s_{t-1}}, {h_i})$
* **Weights**: The weights ${\alpha_{t,i}}$ are computed by applying a softmax operation to the previously computed alignment scores:</br> ${\alpha_{t,i}} = softmax({e_{t,i}})$
* **Context Vector**: A unique context vector, ${c_t}$, is fed into the decoder at each time step. It is computed by a weighted sum of all, T, encoder hidden states: ${c_t} = \sum_{i=1}^{T} \alpha_{t,i}h_{i}$

Self attention involves evaluating attention with respect to oneself, i.e. how relevant is the ith word in sentence relevant to other words in the sentence. The above attention mechanism can be generalized and can be seen as retrievel of a value $V_{i}$ for a query $Q_{i}$ based on a key $K_{i}$ in a database. The general attention mechanism perform the following computations:
* Each query vector $Q_{i} = s_{i-1}$ is matched against a database of keys to compute a score value. The matching operation is computed as the dot product of the specific query under consideration with each key vector $K_{i}$
* The score are passed through a softmax operation to generate weights: $\aplha_{q,k_{i}} = softmax(e_{q,k_{i}})$
* The generalized attention is then computed by a weighted sum of the value vectors $V_{k_{i}}$, where each value vector is paired with a corresponding key: $attention(Q, K, V) = \sum_{i} \alpha_{q, k_{i}, v_{k_{i}}}$

Each word in an input sentence would be attributed its own query, key, and value vectors. This allows to essentially compare a hidden vector for output word to each one of hidden vector for an input word and then combine them together to produce a context vector that reflects what are the words of interest in translating next word.

The multi-head attention is going to predict attention between every position with respect to every other position hence we will have vectors that embed the words in each one of those positions. Then we simply carry out attention computation that treat each word as a query and then find some key that correspond to the other words in the sentence and then take a convex combination of the corresponding value.
When we compute attention we look at pairs together in one block and then repeat it multiple times (n times) hence we have n stacks of blocks having the pair of pairs combination.
