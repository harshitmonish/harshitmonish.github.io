---
title: 'What are Transformer models?'
date: 2022-10-01
permalink: /posts/2022/10/blog-post-3/
tags:
  - Artificial Intelligence
  - Machine Learning
  - Transformer model
---
Transformer Model Introduction
===
<figure>
<img src='/images/transformers.png' width="75%" height="50%">
<figcaption align="center"><b> The  encoder-decoder structure of the Transformer architecture taken from <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></b></figcaption>
</figure>

A transformer model is a deep neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence. It adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. 
Like recurrent neural networks (RNNs), LSTMs,  transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs that are slow to train and the input data needs to be passed sequentially, transformers process the entire input all at once in parallel. 

It applies an evolving set of mathematical techniques called attention or self-attention to detect subtle ways even distant data elements in a series depend on each other. 
Transformers were introduced in 2017 in the paper "[Attention is All You Need](https://arxiv.org/abs/1706.03762)" by a team at Google Brain and are increasingly the model of choice for NLP problems, replacing RNN models such as long short-term memory (LSTM). Transformer network architecture employs a encoder decoder architecture much like RNNs but, it allows to pass the input sequence in parallel which allow training parallelization allows on larger datasets. The task of the encoder is to map an input sequence to a sequence of continuous representations which is fed into decoder and the decoder receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence.

Deep Dive into Transformer model blocks
===
## Positional Encoding ##
First thing that we do is generate input embeddings of the text to map every word to a point in space where similar words are physically closer to each other. We could use pretrained embeddings of words to save time i.e. GLOVe, ELMO embeddings. The paper uses the sine and cosine function to generate the word embeddings. This embedding maps word to a vector, same word may have different meaning in different context in a sentence and this is where positional embedding comes into play. If we don't use positional encoding then we are treating all words as bag of words instead of sequence of words. After having input embedding and applying the positional encoding we get the word vectors that have positional information i.e. context.

## Attention in Machine Learning ##
Before getting into the details of the encoder block, let's talk first about the concept of attention. Attention is the ability to dynamically highlight and use the salient parts of the information at hand, it involves answering what part of input should I be focusing on - in a similar manner as it does in human brain. It was introduced by [Bahdanau et al. (2014)](https://arxiv.org/abs/1409.0473) to address the bottleneck problem that arises with the use of a fixed-length encoding vector, where the decoder would have limited access to the information provided by the input. The attention mechanism described in the paper is divided into three steps:
* Alignment scores: It takes the encoded hidden states ${h_i}$ and the previous decoder output, ${s_i-1}$ to compute a score ${e_t,i}$ that indicates how well the elements of the input sequence align with the current output at the position, t. The alignment model is represented by a function, which can be implemented by a feedforward neural network: ${e_t,i} = a({s_t-1}, {h_i})$  

Why AI Accelerators?
===
Now that we have established the core concepts of CPUs and GPUs, let's talk about the need of AI accelerators. Dedicated AI processors have a large dedicated compute to general-compute ratio. They are ideal for workloads with predictable memory access patterns, as they can provide the highest performance per power when the data is properly pipelined to compute. GPUs originally architecture for graphics but because of parallelism that they have, they are pretty efficient to compare to using just CPUs.

**Since GPU is already an efficient machine for model training, what does AI accelerators do better?** 
In neural networks, It's either matrix-matrix multiplication or matrix-vector multiplication that needs to be done very efficiently. Hence, a matmul engine which is very efficient and a cluster of tensor processor cores with custom instruction set and AI centric operations boosts the performance overall. The combination of fully programmable tensor-cores and a very efficient matmul engine allows us to get to really high efficiency in terms of computing and memory resources.

Which hardware is best for neural networks?
===
While there are multiple different types of processors, they are becoming more heterogeneous and are actually beginning to share more common characteristics. Given the growing complexity of neural networks, the right processor is one that balances general purpose compute and specialized compute and scales well across nodes. In general, if you are working with variety of complex workloads or a starting point when exploring an AI solution, a more general purpose CPU processor especially one with built-in acceleration is likely the best choice.

If you require the highest performance per power with predictable memory access patterns, or you require something more specialized then a dedicated accelerator can likely be the best choice. And a vital consideration is that the processor is simple to program and to use.
