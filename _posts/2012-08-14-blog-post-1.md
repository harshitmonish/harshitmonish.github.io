---
title: 'Deep Dive into AI Infrastructure Stack'
date: 2012-08-14
permalink: /posts/2012/08/blog-post-1/
tags:
  - AI
  - Infrastructure
  - Software, Hardware
---

Artificial Intelligence and Machine Learning rapid uptake over recent years can be attributed not only to new software platforms helping to orchestrate AI and recent advancements in AI models, but also to advancements in the core hardware enabling information processing across massive volumes of data. This has lead to more attraction and new possibilities to build and deploy AI applications in a streamlined fashion. 
With the rise of GPU (general-purpose parallel processing) as well as AI-focused ASICs (application-specific processing), like TPUs (Tensor Processing Units), engineers today are able to analyze large amount of data in a cost-effective and scalable manner and answer high-impact operational questions.

AI Infrastructure stack can be broadly categorized in 4 major layers:

![alt text](https://github.com/harshitmonish/harshitmonish.github.io/blob/main/images/Blog-1.PNG)

* The first layer is called the application layer where we execute our AI application software.
* The second layer is called the middleware and framework layer which comprise AI and ML frameworks e.g. Tensorflow, PyTorch, Keras, etc.
* The third layer is called the programming model layer and is the lowest layer of the software that is closest to the hardware and interacts with the layer above to create optimized code for the specific architecture being targeted, e.g. x86, ARM, etc. This layer is majorly written by library developers having expertise in hardware/ software interaction. This is the layer where AI specific hardware features and interactions are leveraged to accelerate AI software.
* The fourth layer is the hardware layer which comprise either CPU, GPU, AI accelerators for execution of AI software code.




Headings are cool
======

You can have many headings
======

Aren't headings cool?
------