---
title: 'What are Transformer models?'
date: 2022-10-01
permalink: /posts/2022/10/blog-post-3/
tags:
  - Artificial Intelligence
  - Machine Learning
  - Transformer model
---
Transformer Model Introduction
===
<figure>
<img src='/images/transformers.png' height="50">
<figcaption align="center"><b> The  encoder-decoder structure of the Transformer architecture taken from <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></b></figcaption>
</figure>

A transformer model is a deep neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence. It adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. 
Like recurrent neural networks (RNNs), LSTMs,  transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs that are slow to train and the input data needs to be passed sequentially, transformers process the entire input all at once in parallel. 

It applies an evolving set of mathematical techniques called attention or self-attention to detect subtle ways even distant data elements in a series depend on each other. 
Transformers were introduced in 2017 in the paper "[Attention is All You Need](https://arxiv.org/abs/1706.03762)" by a team at Google Brain and are increasingly the model of choice for NLP problems, replacing RNN models such as long short-term memory (LSTM). Transformer network architecture employs a encoder decoder architecture much like RNNs but, it allows to pass the input sequence in parallel which allow training parallelization allows on larger datasets. The task of the encoder is to map an input sequence to a sequence of continuous representations which is fed into decoder and the decoder receives the output of the encoder together with the decoder output at the previous time step to generate an output sequence.

Deep Dive into Transformer model blocks
===
## Positional Encoding ##
First thing that we do is generate input embeddings of the text to map every word to a point in space where similar words are physically closer to each other. We could use pretrained embeddings of words to save time i.e. GLOVe, ELMO embeddings. The paper uses the sine and cosine function to generate the word embeddings. This embedding maps word to a vector, same word may have different meaning in different context in a sentence and this is where positional embedding comes into play. If we don't use positional encoding then we are treating all words as bag of words instead of sequence of words. After having input embedding and applying the positional encoding we get the word vectors that have positional information i.e. context.

## Attention in Machine Learning ##
Before getting into the details of the encoder block, let's talk first about the concept of attention. Attention is the ability to dynamically highlight and use the salient parts of the information at hand, it involves answering what part of input should I be focusing on - in a similar manner as it does in human brain. It was introduced by [Bahdanau et al. (2014)](https://arxiv.org/abs/1409.0473) to address the bottleneck problem that arises with the use of a fixed-length encoding vector, where the decoder would have limited access to the information provided by the input. The attention mechanism described in the paper for RNNs is divided into three steps:
* **Alignment scores**: It takes the encoded hidden states ${h_i}$ and the previous decoder output, ${s_{i-1}}$ to compute a score ${e_{t,i}}$ that indicates how well the elements of the input sequence align with the current output at the position, t. The alignment model is represented by a function, which can be implemented by a feedforward neural network: ${e_{t,i}} = a({s_{t-1}}, {h_i})$
* **Weights**: The weights ${\alpha_{t,i}}$ are computed by applying a softmax operation to the previously computed alignment scores: ${\alpha_{t,i}} = softmax({e_{t,i}})$
* **Context Vector**: A unique context vector, ${c_t}$, is fed into the decoder at each time step. It is computed by a weighted sum of all, T, encoder hidden states: ${c_t} = \sum_{i=1}^{T} \alpha_{t,i}h_{i}$

### Generalized Attention ###
<figure>
<img src='/images/transformers_scaled_attention.png'>
<figcaption align="center"><b> Scaled dot product attention taken from <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></b></figcaption>
</figure>
Self attention involves evaluating attention with respect to oneself, i.e. how relevant is the ith word in sentence relevant to other words in the sentence. The above attention mechanism can be generalized and can be seen as retrievel of a value $V_{i}$ for a query $Q_{i}$ based on a key $K_{i}$ in a database. The general attention mechanism perform the following computations:
* Each query vector $Q_{i} = s_{i-1}$ is matched against a database of keys to compute a score value. The matching operation is computed as the dot product of the specific query under consideration with each key vector $K_{i}$
* The score are passed through a softmax operation to generate weights: $\alpha_{q,k_{i}} = softmax(e_{q,k_{i}})$
* The generalized attention is then computed by a weighted sum of the value vectors $V_{k_{i}}$, where each value vector is paired with a corresponding key: $attention(Q, K, V) = \sum_{i} \alpha_{q, k_{i}, v_{k_{i}}}$

Each word in an input sentence would be attributed its own query, key, and value vectors. This allows to essentially compare a hidden vector for output word to each one of hidden vector for an input word and then combine them together to produce a context vector that reflects what are the words of interest in translating next word.

### Multi head attention ###
<figure>
<img src='/images/transformers_multi_head.png'>
<figcaption align="center"><b> The Multi head attention block consisting of several layer attention layers in parallel taken from <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></b></figcaption>
</figure>
The multi-head attention is going to predict attention between every position with respect to every other position hence we will have vectors that embed the words in each one of those positions. Then we simply carry out attention computation that treat each word as a query and then find some key that correspond to the other words in the sentence and then take a convex combination of the corresponding value.
When we compute attention we look at pairs together in one block and then repeat it multiple times (n times) hence we have n stacks of blocks having the pair of pairs combination.

## The Encoder Block ##
The encoder consists of a stack of N = 6 identical layers, where each layer is composed of two sublayers:
* The first sublayer implements a multi-head self-attention mechanism. Followed by a Add & Norm layer that adds a residual connection of input to output of multihead attentin layer and then normalize it (layer normalization).
* The second sublayer is a fully connected feed-forward network that is applied to every one of the attention vectors. These feed forward networks are used in practice to transform the attention vectors into a form that is expected by the next decoder block. Since each of the attention head are independent of each other hence we can leverage parallelization here and send all the words in a sentence at once and output will be a set of encoded vectors for every word.

## The Decoder Block ##
Before getting into the details of decoder block, let's touch upon the topic of masked multi-head attention.

### Masked Multi-Head Attention ###
Masked multi-head attention is similar to multi-head attention where some values are masked(i.e. probabilities of masked values are nullified to prevent them from being selected) when decoding an output value. The output value should only depend on previous outputs(not future outputs), hence we mask future outputs.
$masked attention(Q, K, V) = softmax(\frac{Q^{T}K + M}{\sqrt{d_{k}}})$ where M is the mask matrix.

Coming back to the decoder block, decoder also consists of a stack of N = 6 identical layers that are each composed of three sublayers:
* The first sublayer receives the previous output of the decoder stack, augments it with positional information, and implements multi-head self-attention over it. While the encoder is designed to attend to all words in the input sequence regardless of their position in the sequence, the decoder is modified to attend only to the preceding words. Hence, the prediction for a word at position can only depend on the known outputs for the words that come before it in the sequence. In the multi-head attention mechanism (which implements multiple, single attention functions in parallel), this is achieved by introducing a mask over the values produced by the scaled multiplication of matrices.
* The second layer implements a multi-head self-attention mechanism similar to the one implemented in the first sublayer of the encoder. On the decoder side, this multi-head mechanism receives the queries from the previous decoder sublayer and the keys and values from the output of the encoder. This allows the decoder to attend to all the words in the input sequence.
* The third layer implements a fully connected feed-forward network, similar to the one implemented in the second sublayer of the encoder.

Furthermore, the three sublayers on the decoder side also have residual connections around them and are succeeded by a normalization layer. Positional encodings are also added to the input embeddings of the decoder in the same manner as previously explained for the encoder.

The layer normalization normalize values in each layer to have 0 mean and unit variance for each hidden unit $h_{i}$ computed as: $ h_{i} = \frac{g \times (h_{i} - \mu)}{\sigma} $ where g is a variable and $ \mu = \frac{1}{H} \sum_{i=1}^{H}h_{i}$ and $ \sigma = \sqrt{\frac{1 \times \sum_{i=1}^{H} (h_{i} - \mu)^{2}}{H}} $
This reduces the covariance shift (gradient dependencies between ach layer) and therefore fewer training iterations.